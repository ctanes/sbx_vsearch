# -*- mode: Snakemake -*-

TARGET_VSEARCH = [
    expand(str(MAPPING_FP/'vsearch'/'{sample}_report.tsv'), sample = Samples.keys()),
    expand(str(MAPPING_FP/'vsearch'/'{sample}.fasta'), sample = Samples.keys())
    ]

#TARGET_VSEARCH_REPORT = [str(MAPPING_FP/'vsearch'/'taxonomic_assignments.tsv')]

rule all_vsearch:
    input:
        TARGET_VSEARCH

# Run vsearch 
rule run_vsearch:
    input:
        query = expand(str(QC_FP/'decontam'/'{sample}_{rp}.fastq.gz'),
                      sample = "{sample}",
                      rp = Pairs),
        db_file = Cfg['sbx_vsearch']['db']
    output:
        reports = str(MAPPING_FP/'vsearch'/'{sample}_report.tsv'), sample = Samples.keys(),
        alignments = str(MAPPING_FP/'vsearch'/'{sample}.fasta'), sample = Samples.keys()
    threads:
        Cfg['sbx_vsearch']['threads']

#TODO: all of these below have to be under "params:" like with other rules
iddef:
        Cfg['sbx_vsearch']['iddef']
    min_id:
        Cfg['sbx_vsearch']['min_id']
    userfields:
        Cfg['sbx_vsearch']['userfields']
    weak_id:
        Cfg['sbx_vsearch']['weak_id']
    shell:
        """
            vsearch --usearch_global $(cat {input.query[0]} {input.query[1]}) \
            --db {input.db_file} \
            --userout {output.reports} \
            --matched {output.alignments} \
            --threads {threads} \
            --iddef {iddef} \
            --id {min_id} \
            --userfields {userfields} \
            --weak_id {weak_id}
        """

#TODO: summary of results
# An all-samples-in-one summary table, with samples on columns and taxa on
# rows.
#rule taxonomic_assignment_report:
#    """ generate vsearch taxonomic assignment table """
#    output:
#        str(MAPPING_FP/'vsearch'/'taxonomic_assignments.tsv')
#    input:
#        fps = expand(str(MAPPING_FP/'vsearch'/'review'/'{sample}_review.txt'), sample = Samples.keys())
#    run:
#        vsearch_make_report(input.fps, output[0])
#


#TODO: use this as a template for making a --makeudb_usearch
# Make sure vsearch has downloaded its database and indexed it for bowtie2,
# storing the files inside the Sunbeam environment in opt/vsearch_databases,
# and symlink to it in the vsearch output dir.
#rule vsearch_database:
#    output:
#        dbdir = str(MAPPING_FP/'vsearch'/'databases')
#    conda:
#        "sbx_vsearch_env.yml"
#    params:
#        conda_prefix = CONDA_PREFIX
#    shell:
#        """
#            # Snakemake by default fails the whole shell script if any command
#            # within a pipe has a non-zero exit status.  Disabling that here.
#            set +o pipefail
#
#            # Link the two possible database locations (later versions of
#            # vsearch2 use "vsearch_databases") from the metphlan
#            # environment back out to the Sunbeam environment.
#            # This allows easier access to the database on disk, whether it's
#            # supplied automatically by vsearch2 or manually before running
#            # these rules.
#            path_db="{params.conda_prefix}/opt/vsearch_databases"
#            mkdir -p "$path_db"
#            ln -sfT "$path_db" "$CONDA_PREFIX"/bin/databases
#            ln -sfT "$path_db" "$CONDA_PREFIX"/bin/vsearch_databases
#
#            function lsbt2 {{
#            ls -1 "$CONDA_PREFIX"/bin/databases/mpa_v20_m200{{.{{1..4}},.rev.{{1..2}}}}.bt2 2> /dev/null | wc -l
#            }}
#            bt2=$(lsbt2)
#            if [[ "$bt2" != 6 ]]
#            then
#                vsearch2.py --install
#            fi
#            # To track completion of this task, place a symlink as an output
#            # file pointing into the database location inside the vsearch
#            # environment.
#            bt2=$(lsbt2)
#            if [[ "$bt2" == 6 ]]
#            then
#                ln -sf "$CONDA_PREFIX/bin/databases" "{output.dbdir}"
#            fi
#        """

##################### Helper functions
#
#def vsearch_make_report(fps_input, fp_output):
#    kos = []
#    for fp in fps_input:
#        # filter out the files that don't have any results
#        if os.path.getsize(fp) > 1:
#            # build pandas dataframes for each file of results
#            kos.append(parse_results(fp,".txt"))
#    # merge the column results
#    kos = pandas.concat(kos, axis=1)
#
#    # write them to file. Replace NAs (due to merging) with 0.
#    kos.to_csv(fp_output, sep='\t', na_rep=0, index_label="Term")
#
#def parse_results(fp, input_suffix):
#    """ Return a DataFrame containing the results of one sample"""
#    sample_name = os.path.basename(fp).rsplit(input_suffix)[0]
#    return pandas.read_csv(fp, sep='\t', index_col=0, names=[sample_name], skiprows=1)
#
## Chunyu's function to filter to just species-level classifications, excluding
## strain-level.
#def review_output(raw_fp, output_fp):
#     with open(raw_fp) as f:
#        output_lines = f.read().splitlines(True)
#        
#        if len(output_lines) < 2:
#            raise ValueError("Metaphlan output has fewer than 2 lines.")
#        elif len(output_lines) == 2:
#            revised_output_lines = "".join(output_lines)
#        else:
#            header = output_lines.pop(0)
#            revised_output_lines = [header]
#            for line in output_lines:
#                if (("s__" in line) and not ("t__" in line)) or (("unclassified" in line) and not ("t__" in line)):
#                    revised_output_lines.append(line)
#            revised_output_lines = "".join(revised_output_lines)
#        
#        with open(output_fp, 'w') as f:
#            f.write(revised_output_lines)
